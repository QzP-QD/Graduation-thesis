**写在前面：**
**本文翻译供个人研究学习之用，不保证严谨与准确**   
**github链接：https://github.com/WithHades/network_traffic_classification_paper**  
**本文原文：Lopez-Martin, M., Carro, B., Sanchez-Esguevillas, A., & Lloret, J. (2017). Network Traffic Classifier with Convolutional and Recurrent Neural Networks for Internet of Things. IEEE Access, 5, 18042–18050. https://doi.org/10.1109/ACCESS.2017.2747560**

<!-- TOC -->

- [基于卷积和递归神经网络的物联网流量分类器](#基于卷积和递归神经网络的物联网流量分类器)
  - [I. 介绍](#i-介绍)
  - [II. 相关工作](#ii-相关工作)
  - [III. 工作描述](#iii-工作描述)
    - [A. 选择数据集](#a-选择数据集)
    - [B. 模型描述](#b-模型描述)
    - [IV. 结果](#iv-结果)
    - [A. 网络架构的影响](#a-网络架构的影响)
    - [B. 特征的影响](#b-特征的影响)
    - [C. 时间序列长度的影响](#c-时间序列长度的影响)
  - [V. 结论](#v-结论)

<!-- /TOC -->

# 基于卷积和递归神经网络的物联网流量分类器
  
> ***摘要***  
> &emsp;&emsp;网络流量分类器（NTC）是当前网络监控系统的重要组成部分，它的任务是了解通信流（如HTTP和SIP）当前使用的网络服务。该检测基于与通信流相关联的多个特征，例如，源端口和目标端口以及每个包传输的字节。NTC很重要，因为只要知道网络服务（所需的延迟、流量和可能的持续时间），就可以了解和预测当前网络流的许多信息。这对于物联网（IoT）网络的管理和监控尤为重要，NTC将有助于隔离异构设备和服务的流量和行为。本文提出了一种新的基于深度学习模型的物联网流量控制技术。结果表明，递归神经网络（RNN）与卷积神经网络（CNN）相结合，具有较好的检测效果。CNN的自然域，即图像处理，已经以一种简单自然的方式扩展到NTC。结果表明，该方法在不需要任何特征工程的前提下，比其他算法具有更好的检测效果。本文在几个集成CNN和RNN的架构上，包括所选特征的影响和用于训练的网络流的长度方面进行了全面的研究。  

> &emsp;&emsp;***关键词：*** **卷积神经网络**，**深度学习**，**网络流量分类**，**循环神经网络**

## I. 介绍
&emsp;&emsp;网络流量分类器（NTC）是当前网络管理系统的重要组成部分。NTC推理网络流所使用的服务/应用程序（例如HTTP、SIP…）。此信息对于网络管理和服务质量（QoS）很重要，因为所使用的服务与QoS要求和用户合同/期望有直接关系。  
&emsp;&emsp;很明显，物联网（Internet of Things，IoT）流量将对当前的网络管理和监控系统构成挑战，因为连接设备的数量庞大且异构。NTC是这个新场景中的关键组件[1]，[2]，其允许检测具有非常不同用户配置文件的不同设备使用的服务。在物联网中，网络流量识别对于实现网络策略和资源的有效管理至关重要，因为网络需要根据流量概况信息做出不同的反应。  
&emsp;&emsp;有很多地方实现NTC：基于端口，基于payloadbased和基于流统计的[3]，[4]。基于端口的方法利用端口信息服务标识。这些方法不可靠，因为许多服务不使用已知端口，甚至不使用其他应用程序使用的端口。  
&emsp;&emsp;基于有效载荷的方法是通过对通信流执行的有效载荷进行深度包检查（DPI）来解决该问题。这些方法在包中寻找已知的模式。他们目前提供了最好的检测率，同时也面临着一些相关的成本和困难：依赖最新模式数据库（必须维护）的成本，以及访问原始有效负载的困难。目前，越来越多的传输数据被加密或需要保证用户隐私策略，这是基于有效载荷的方法的一个实际问题。  
&emsp;&emsp;最后，基于流统计的方法依赖于可以从包头获得的信息（例如，发送的字节、包的间隔时间、TCP窗口大小…）。它们依赖于包头高级信息，这使得它们成为处理不可用的有效负载或动态端口的更好选择。这些方法通常依赖机器学习技术来执行服务预测[3]。在这种情况下，有两种机器学习方法可供选择：有监督和无监督方法。监督方法通过训练一个包含真实标记输出的样本的算法来学习一组特征和期望的标记输出之间的关联。在无监督方法中，我们没有相关的真实标记输出的数据，因此，他们只能根据一些内在的相似性尝试将样本分组（聚类）。  
&emsp;&emsp;本文提出了一种新的基于流量统计的监控方法来检测IP网络流量所使用的服务。该方法利用了在流生存期内从交换包的报头中提取的若干特征。对于每个流，我们构建一个特征向量的时间序列。时间序列的每个元素都将包含流中数据包的特征。同样地，每个流都有一个相关的服务/应用程序（一个标签值），这是训练算法所必需的。 
&emsp;&emsp;为了确保数据的机密性，我们的方法只使用数据包头的特性，不包括IP地址。  
&emsp;&emsp;为了训练这种方法，我们使用了25万多个网络流，其中包含100多个不同的服务。另一个挑战是，这些服务的频率分布极不平衡。  
&emsp;&emsp;该方法是一种基于卷积神经网络（CNN）和递归神经网络（RNN）相结合的深度学习模型的分类器。  
&emsp;&emsp;这项工作的主要推动因素之一是评估深度学习进展对NTC问题的适用性。因此，我们研究了不同深度学习体系结构的充分性以及几个设计决策的影响，例如所选择的特征或分析中包含的每个流的数据包数。  
&emsp;&emsp;在本文中，我们比较了不同体系结构的性能结果，特别是考虑了RNNs单独、CNNs单独以及CNN和RNN的不同组合。  
&emsp;&emsp;为了将CNN应用于特征向量的时间序列，我们提出了一种将数据呈现为相关伪图像的方法，可以应用CNN。  
&emsp;&emsp;在评估新方法的适用性时，将其应用于实际数据是很重要的。我们利用了来自西班牙学术和研究网络RedIRIS的数据。  
&emsp;&emsp;论文的结构安排如下：第II节介绍了相关工作。第III节介绍了所做的工作。第IV节介绍了所得结果，最后，第V节给出了讨论和结论。  

## II. 相关工作
&emsp;&emsp;在NTC中，由于所研究的数据集和所应用的性能指标有很大的不同，比较工作结果是很困难的。NTC本质上是一个多类分类问题。如第IV节所述，目前还没有一个统一的度量来表示多类问题的结果。考虑到这些事实，我们现在提出了一些相关的工作。  
&emsp;&emsp;神经网络在NTC中的应用已有许多工作，但所采用的网络模型在本质上与本文所提出的网络模型有很大的不同。  
&emsp;&emsp;在文献[5]中，他们提出了一种具有零层或一层隐藏层的多层感知器（MLP），但实际上它是作为内部架构来应用完全贝叶斯分析的。对于10个分组标签，使用246个特征的最佳精度为99.8%，宏平均精度为99.3%（10个标签）。  
&emsp;&emsp;文[6]中应用了一组带纠错输出码的MLP分类器，平均总精度（5个标签）为93.8%。同时，在文献[7]中，采用粒子群优化算法的MLP算法对6个标签进行分类，最佳分类率为96.95%。[8]的目的是研究用于网络流量可视化的神经投影技术。为此，他们提出了几种使用神经网络的降维方法。不执行分类。另一项工作[9]探索了粗糙神经网络在处理不确定性时的适用性，但没有为NTC提供任何性能结果。  
&emsp;&emsp;Zhou等人[10]将具有3个隐藏层和不同数量隐藏神经元的MLP应用于Moore数据集[11]。对于10个类中的一组标签，它们给出的总体准确度大于96%，从而导致最终的类别分布非常不平衡（最高频率类的频率几乎为90%），不提供F1分数。文[12]中使用了一种并行神经网络分类器结构。它由径向基函数神经网络的并行块组成。为了训练网络，采用了一个负向强化学习算法。分类6个标签的总体准确率95%，未提供F1分数。  
&emsp;&emsp;另一组论文将与神经网络无关的一般机器学习技术应用于NTC问题。  
&emsp;&emsp;Kim等人[13] 提出了一种基于熵的特征最小描述长度离散化方法，作为C4.5、朴素贝叶斯、支持向量机和kNN等算法的预处理步骤。声称提高了算法的性能，11个分组标签的一对多准确率达到93.2%-98%。[14]中作者使用了不同的机器学习算法实现NTC（C4.5，支持向量机，朴素贝叶斯），采取了23个特征，实现了五项服务的分类（www，dns，ftp，p2p，telnet），最终的平均精度低于80%。  
&emsp;&emsp;Wang等人[15] 使用一个有29个选定特征的增强随机森林。它们将服务分组为12个类，只提供一对多度量（不聚合）。F1成绩在0.3-0.95之间，只有3个等级高于0.96。他们使用自己的数据集。[16]的作者将流量相关性纳入半监督模型中，该模型的总体准确度小于85%，10个标签的一对多F1得分小于0.9（除了两个标签分别为0.95和1）。他们使用WIDE backbone数据集[16]。他们使用C4.5、kNN、Naive Bayes、贝叶斯网络和Erman的半监督方法[17]–[21]取得了比其他工作更好的结果。  
&emsp;&emsp;文[22]提出了一种有向无环图支持向量机，平均精度为95.5%。该方法应用于由剑桥大学（Moore数据集）提供的数据集的类的一对一组合[11]。Yamansavassilar等人[23]研究了几种算法在UNB-ISCX网络流量数据集中的应用：J48、Random-Forest、Bayes-Net和kNN，共有14类12个特征，最佳精度为93.94%。  
&emsp;&emsp;Yuan和Wang[24]在Hadoop平台上提出了一种决策树算法C4.5的变体。他们对12个标签进行分类，一对一的准确度在60-90%的区间内，只有两个标签高于90%。数据集是剑桥大学的Moore数据集[11]。  
&emsp;&emsp;本文介绍了RNN和CNN模型在NTC问题中的首次应用。两种模型的结合提供了网络流的自动特征表示，而不需要昂贵的特征工程。

## III. 工作描述
&emsp;&emsp;以下各节介绍了用于此项工作的数据集，并描述了所应用的不同深度学习模型。

### A. 选择数据集
&emsp;&emsp;在这项工作中，我们使用了来自RedIRIS的真实数据。RedIRIS是西班牙学术和研究骨干网络，为科学界和国家大学提供先进的通信服务。RedIRIS拥有500多个附属机构，主要是大学和公共研究中心。  
&emsp;&emsp;我们从RedIRIS中提取了266160个网络流。这些流包含108个不同的标记服务，具有高度不平衡的频率分布。图1示出了15个最频繁服务的名称和频率分布。频率分布基于具有特定服务的流的比例。  
&emsp;&emsp;网络流包括共享源和目标IP地址、端口号以及传输协议的唯一双向组合的所有数据包：TCP或UDP。包括加密的包，因为所考虑的算法不依赖有效负载内容。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622104121.png" div align=center/> 
</div> 
<center>图1 15个最频繁服务的频率分布</center>

&emsp;&emsp;每个流都与特定的服务相关联。为了训练和评估模型，我们需要为每个流分配一个真实标签。此分配最初不可用，并且通过将nDPI工具[25]应用于流生存期期间交换的数据包而成为可能。nDPI应用DPI技术来执行服务检测。DPI通过检查包的报头和有效负载来提供最佳的可用分类结果。考虑到这一点，我们假设DPI工具的输出是对真实标签的最佳近似。nDPI处理加密的流量，它是最精确的开源DPI应用程序[26]。nDPI无法标记的流被丢弃。对于这项工作，我们考虑了UDP和TCP流。  
&emsp;&emsp;每个流由多达20个包的序列形成。对于包，我们提取了以下六个特征：源端口、目标端口、包有效负载中的字节数、TCP窗口大小、包的间隔时间和方向。对于UDP数据包，TCP窗口大小（TCP流控制）设置为零。包地址可以具有0-1的值，该值指示包是从源到目的地还是朝相反的方向。  
&emsp;&emsp;我们只考虑了流生命周期中交换的前20个包。对于超过20个数据包的流，我们丢弃了20个数据包之后的任何数据包。正如我们将看到的，20个数据包足以获得异常检测率，而且更少数量的包仍然提供出色的性能。  
&emsp;&emsp;最后，根据这些流，我们构建了数据集。因此，数据集由266160个流组成，每个流包含20个向量的序列，每个向量由6个特征（从包的头部提取的6个特征）组成。最终结果是与每个流相关联的特征向量的时间序列。  
&emsp;&emsp;为了评估模型，我们将15%的流作为验证集。本文给出的所有性能指标都对应于该验证集。为了构建验证集，我们对原始流进行采样，保持验证集和剩余流（训练集）之间相同的标签频率。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622104513.png" div align=center/> 
</div> 
<center>图2 网络流量组成</center>

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622104538.png" div align=center/> 
</div> 
<center>图3 RNN模型</center>

&emsp;&emsp;图2示出了数据集中的网络流的最终情况。

### B. 模型描述
&emsp;&emsp;研究了不同的深度学习模型。具有最佳检测性能的模型是CNN[27]和RNN[28]的组合。在本节中，我们将展示为这项工作考虑的所有模型。
&emsp;&emsp;分析的第一个模型（图3）是一个简单的RNN。特别是，我们使用了一种称为LSTM[29]的RNN变体，它更容易训练（它解决了消失梯度问题）。LSTM由两个维度的值矩阵训练：时间维度和特征向量。LSTM使用时间序列特征向量和与其内部隐藏状态和单元状态相关联的两个附加向量迭代一个神经网络（单元）。单元格的最终隐藏状态对应于输出值。因此，LSTM层的输出维度与其内部隐藏状态（LSTM单元）的大小相同。在图3的模型中，我们在最后添加了几个完全连接的层。当前一层的每个节点完全向前连接到连续节点的每个节点时构成两层完全连接层。所有模型都添加了全连接层。
&emsp;&emsp;在图4中示出了纯CNN网络。CNNs最初被应用于图像处理，作为一个生物启发的模型来执行图像分类，其中特征工程是由网络自动完成的，这得益于从图像中提取位置不变模式的内核（过滤器）的作用。链接多个cnn能够自动提取复杂特征。
&emsp;&emsp;在我们的例子中，我们使用了这个图像处理隐喻来将该技术应用于一个非常不同的数据集。为此，我们将特征向量的时间序列所形成的矩阵看作一幅图像。图像像素是局部相关的；类似的，与连续时隙相关联的特征向量呈现相关的局部行为，这使得我们可以采用这种类比。  

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622111551.png" div align=center/> 
</div> 
<center>图4 深度学习的CNN模型</center>

&emsp;&emsp;每个CNN层生成一个多维数组（张量），其中图像的维数减小，但同时生成一个新的维数，使这个新维数的大小等于应用于图像的滤波器的数量。连续的CNN层将进一步降低图像尺寸，并增加新生成的尺寸大小。为了填补模型的不足，有必要将张量转换成一个向量，这个向量可以作为最终全连接层的输入。为了完成这个转换，可以做一个简单的张量展平（图4）。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622111614.png" div align=center/> 
</div> 
<center>图5 深度学习的CNN和单独层RNN模型的结合</center>

&emsp;&emsp;先前的模型可以组合成如图5所示的单个模型。在这个组合模型中，几个链式cnn的最终张量被重塑成一个矩阵，可以作为RNN（LSTM网络）的输入。为了将张量重塑为一个矩阵，我们保持与滤波器相关联的维数不变，在其他两个维度上执行，最终得到一个矩阵形状。最后一个CNN的滤波器产生的值将与特征向量等价，并且由整形操作产生的平坦向量将充当LSTM层所需的时间维度。  
&emsp;&emsp;最后，图6中引入的模型与前一个模型相似，包括附加的LSTM层。当几个LSTM层被连接时，LSTM行为与前面解释的不同（图3）。在这种情况下，所有LSTM层（除了最后一层）都采用“返回序列”模式，生成对应于递归网络的连续迭代的向量序列。这个向量序列可以按时间序列分组，形成下一个LSTM层的入口点。需要注意的是，对于连续的LSTM层，数据输入的时间维度没有改变（图6），但是连续输入的向量维度改变了。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622111814.png" div align=center/> 
</div> 
<center>图6 深度学习的CNN和两层RNN模型的结合</center>

&emsp;&emsp;此外，对于前面介绍的不同类型的层，我们还使用了一些附加层： batch
normalization、max pooling和dropout layers。
&emsp;&emsp;dropout layers[30]通过dropout（设置为零）前一层的输出百分比，提供正则化（未看到数据的结果的泛化）。这种行为迫使网络不过度依赖任何特定的输入，能够有效防止过拟合，提高泛化能力。
&emsp;&emsp;Max pooling[31]是一种卷积层。区别在于使用的过滤器。在max pooling中，它使用max filter，它选择应用过滤器的图像区域的最大值。它减少了输出的空间大小，减少了网络的特征数目和计算复杂度。结果是一个下采样输出。与dropout层类似，最大池化层提供正则化。
&emsp;&emsp;Batch normalization[32]使训练收敛更快，并且可以提高性能结果。这是通过在训练时规范化处理的每个特征（将输入缩放为零平均值和单位方差）并考虑到整个训练集进行重新校准。在批处理级别上获得新学习的平均值和方差。

### IV. 结果
&emsp;&emsp;本节介绍将几个深度学习模型应用于NTC时获得的结果。分析了几种重要参数和设计决策的影响因素：模型构架，选择的特征和从网络流中提取的数据包数。
&emsp;&emsp;为了评估不同情况的检测情况，并考虑到标签的高度不均衡分布，我们为每个选项提供了以下性能指标：准确性、精确度、召回率和F1。考虑到所有的度量，F1可以被认为是这个场景中最重要的度量。F1是精确性和查全率的调和平均值，为不平衡数据集的检测性能提供了更好的指示。F1的最佳值为1，最差值为0。  
&emsp;&emsp;我们对准确性、F1、精确度和召回的定义基于以下四个先前的定义：（1）假阳性FP指实际中未检测到，但事实上属于该分类；（2）假阴性：事实上不属于该分类，但检测中属于该分类；（3）真阳性：检测属于该分类，事实上确实属于该分类；（4）假阳性：检测不属于该分类，事实上确实不属于该分类。考虑到先前的定义：
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\tag{1}$$
$$Precision = \frac{TP}{TP + FP}\tag{2}$$
$$Recall = \frac{TP}{TP + FN}\tag{3}$$
$$F1 = 2\frac{Precision × Recall}{Precision + Recall}\tag{4}$$
&emsp;&emsp;我们使用Tensorflow实现了所有的模型，而sci-kit包计算了了性能度量。所有计算均在商用PC机（i7-4720-HQ，16GB RAM）中进行。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622113344.png" div align=center/> 
</div> 
<center>表1 深度学习网络模型在NTC问题中的应用</center>

### A. 网络架构的影响
&emsp;&emsp;我们尝试了不同的深度学习架构模型，以了解它们是否适合NTC问题。为了构建不同的体系结构，我们考虑了RNN和CNN的不同组合：仅RNN，仅CNN，以及CNN后接RNN的各种安排。在所有情况下，我们都在末尾添加了两个全连接层。 
&emsp;&emsp;在表1中，我们提供了不同架构的描述，在图7中，我们给出了它们的性能度量。从图7可以看出，模型CNN+RNN-2a在精度和F1方面都给出了最好的结果。  
&emsp;&emsp;表I中提供的架构描述如下：Conv（z，x，y，n，m）表示具有z滤波器的卷积层，其中x和y是2D滤波器窗口的宽度和高度，如果m等于S，则步长为n，填充相同；如果m等于V，则有效填充（有效表示没有填充，相同表示保留的填充输出尺寸）。Max pool（x，y，n，m）表示一个Max Pooling层，其中x和y是池大小，如果m等于S，则步长为n，填充相同；如果m等于V，则有效填充（有效表示没有填充，相同表示保留输出维度的填充）。BN代表一个批量规范化层。FC（x）表示具有x个节点的完全连接层。LSTM（x）表示一个LSTM层，其中x是输出空间的维数；在顺序出现多个LSTM的情况下，除最后一个LSTM外，每个LSTM将返回连续的递归值，这些值将作为下一个LSTM的输入值。DR（x）表示dropout系数等于x的dropout层。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622113614.png" div align=center/> 
</div> 
<center>图7 分类性能度量（聚合）与网络模型</center>

&emsp;&emsp;在所有情况下，训练都是在60-90个阶段之间进行的，如果最后10个阶段没有改善损失，则提前停止。在训练过程中，我们考虑了一个复杂数据的epoch传递。  
&emsp;&emsp;所有的激活函数都是ReLU，除了最后一层有Softmax激活。损失函数为Softmax交叉熵，用Adam进行批量随机梯度下降（SGD）优化。  
&emsp;&emsp;在表I中，模型名的后缀“a”意味着模型只改变了dropout层的dropout百分比。  
&emsp;&emsp;最佳模型的准确度为0.9632，F1评分为0.9574，约为0.9543，预测值为0.9632（CNN+RNN-2a模型）。  
&emsp;&emsp;从分析结果可以看出，一个简单的模型，即两个CNN层，然后是一个LSTM层，最后是两个全连接层，在精度和F1方面都提供了最佳的检测结果。包含MaxPooling或其他CNN或LSTM层并不能改善结果。CNN层之间的批处理规范化和在网络末端包含一些dropout层确实提高了结果。  
&emsp;&emsp;对于这个问题，我们有108个不同的服务标签需要检测。这是一个多类分类问题。在这种情况下，有两种可能的方法可以给出结果：聚合结果和一对多结果。  
&emsp;&emsp;对于一对多，我们专注于一个特定的类（label），并将其他类视为一个单独的替代类，将问题简化为每个特定类（逐个）的二进制分类任务。在聚合结果的情况下，我们尝试为所有类提供摘要结果。有不同的方法来执行聚集（微观、宏观、样本、加权），平均过程的方式不同。  
&emsp;&emsp;图7中的性能度量是使用加权平均值的聚合度量。我们使用scikit learn[33]提供的加权平均值来计算F1、精确度和召回分数的总和。  
&emsp;&emsp;在图8中，我们提供了前15个更频繁标签的分类的一对一度量（结果由modelcnn+RNN-2a获得）。图8中的一个重要观察是，对于频率高于1%（图1）的所有标签，我们的准确率始终高于98%，许多情况下高于99%，F1分数高于0.96。这15个标签的宏观平均准确度为99.59%（文献中最佳值）。  

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622114102.png" div align=center/> 
</div> 
<center>图8 多个服务标签分类的性能指标（一个与多个比较）（15个以上）</center>

### B. 特征的影响
&emsp;&emsp;在表2中，我们可以看到学习过程中使用的特征的影响。图2给出了全套可能的特征，但重要的是要了解哪些特征在检测过程中具有更高的重要性。  
&emsp;&emsp;表2通过分析去除不同特征时的检测指标来显示特征的重要性。表2中的第一列给出了用于训练模型（按特征集分组）的功能，右列给出了检测过程的常用聚合性能度量。图9中的图表以不同的格式呈现相同的结果，以便于比较不同的特征集。  

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622114309.png" div align=center/> 
</div> 
<center>表2 分类性能指标（聚合）与采用的特征（模型CNN+RNN-2a）</center>

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622114342.png" div align=center/> 
</div> 
<center>图9 分类性能指标（聚合）与采用的特征（型号CNN+RNN-2a）</center>

&emsp;&emsp;如所料，一般来说，特征越多，效果越好。但是，有趣的是，当数据包到达时间（时间戳）被添加到完整的特性集时，结果会稍差一些。它似乎提供了一些与源端口和目标端口不完全一致的信息，因为一旦我们去掉源端口和目标端口，很明显它又成为一个重要的特性。同样有趣的是，要理解特征TCP窗口大小（WIN-size）的重要性，在使用减少的特征集操作时，它比时间戳更重要。  
&emsp;&emsp;表2提供了带有颜色代码的度量值，以便于对结果进行排序。在这个颜色代码中，深绿色意味着更好的结果，而深红色意味着更差的结果。  
&emsp;&emsp;模型CNN+RNN-2a用于获得表2所示的结果，但当与其他模型重复相同的研究时，结果与特征集之间保持相同的关系。

### C. 时间序列长度的影响
&emsp;&emsp;在分析网络流量时，需要研究的一个重要参数是要考虑的包数的影响。有数百个包的流，而其他的只有一个包。  
&emsp;&emsp;在研究开始时，一个重要的疑问是这个参数的可能影响，因为增加包含的包的数量可以改进检测，但是要花费更高的计算时间和资源。作为一个平衡的决定，我们选择了最多20个包。我们只考虑了网络流生命周期中交换的前20个包。对于超过20个包的流，我们忽略了20个包之后的任何包。小于20个数据包的流用零填充。  
&emsp;&emsp;然后，了解数据包数量对整体检测问题的影响，并确认20个数据包是否是一个合理的数字是很重要的。为此，我们分析了不同包数和不同体系结构下的性能。我们在这里给出了两个有代表性的体系结构（RNN-1和CNN+RNN-2a）的结果。  
&emsp;&emsp;我们可以在图10中看到RNN-1架构的结果，并且需要注意的是，在每个流的数据包数小于5之前，通过使用较少的数据包，总体检测质量不会显著改变。因此，考虑流的第一个包就足以获得允许我们推断其服务的大部分信息。在图10中，我们可以容易地理解，当每个流的分组数小于5时，检测质量开始降低。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622114741.png" div align=center/> 
</div> 
<center>图10 体系结构RNN-1的分类性能指标（聚合）与时间序列长度</center>

&emsp;&emsp;图11显示了对架构CNN-RNN-2a的分组数目的影响的结果。该模型支持分组数目的较小减少（该模型在时间维度上需要最小长度为7），但是我们仍然可以理解，无论初始性能如何降低，这种减少并不是随着包数的减少而单调的，事实上，它将包的长度保持在从7到15之间（忽略一些中间的噪声值）。

<div align=center> 
<img src="https://gitee.com/ysybh/image_bed/raw/master/img/20200622114852.png" div align=center/> 
</div> 
<center>图11 CNN+RNN-2a体系结构的分类性能指标（聚合）与时间序列长度</center>

&emsp;&emsp;因此，很明显，尽管最小的数据包数很重要，但不需要大量的数据包（即依赖于体系结构的数据包）。一般来说，在5到15个包之间就足以获得优异的检测结果。  

## V. 结论
&emsp;&emsp;这项工作有助于改进现有网络监控系统中NTC的可用替代方案和能力；特别是针对物联网网络，其中对流量分类要求很高[1]，[2]。  
&emsp;&emsp;据我们所知，以前没有应用RNN和CNN深度学习模型来解决NTC问题。因此，本文提出的工作在本质上是原创的。  
&emsp;&emsp;这项工作提供了一个深度学习模式为NTC提供的可能性的透彻分析。它展示了RNN和CNN模型的性能及其组合。它证明了CNN可以成功地应用于NTC分类，为将CNN的图像处理范式扩展到矢量时间序列数据提供了一种简单的方法（类似于以前对文本和音频处理的扩展[34]，[35]）。  
&emsp;&emsp;基于CNN和RNN的特定组合的模型给出了最佳的检测结果，这些结果比其他已发表的具有替代技术的成果要好。  
&emsp;&emsp;所选择的特征的影响被证明，并且没有必要处理每个流的大量数据包以获得优异的结果：任何数量的数据包高于5-15（这是一个依赖于体系结构的数量）都会给出类似的结果。  
&emsp;&emsp;该方法具有很强的鲁棒性，在100多个不同分类标签的高度不平衡数据集下具有很好的F1检测性能。它可以处理非常少的特性，不需要特性工程。  
&emsp;&emsp;为了训练模型，我们从数据包中提取了高层次的数据。它不需要依赖IP地址或有效载荷数据，这些数据可能是机密或加密的。  
&emsp;&emsp;一个简单的RNN模型已经提供了非常好的结果，但是当RNN模型与先前的CNN模型相结合时，这些结果会得到改善，这一点很有趣。    
&emsp;&emsp;CNN的加入有可能改善结果，这表明了最初的直觉是如何让我们把从网络包特征中提取的向量时间序列作为图像来吸收的，因此CNN是处理相似性质向量时间序列的有效候选者。    
&emsp;&emsp;作为一个深度学习的架构，这样一个新模型的富有成效的来源，我们认为，作为未来的工作，需要试验新的应用和变种的CNN和LSTM模型。  
&emsp;&emsp;这项工作特别适用于新的物联网，其中NTC可用于区分或隔离不同类别的业务，例如设备识别[36]、无线传感器网络（WSN）中的目标检测[37]或基于用户优先级的[38]。  